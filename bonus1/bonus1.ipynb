{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я попробовала делать просто эмбеддинги на GraphCodeBERT и OpenAi Codex -- ну так себе работает на претрейне, они не готовы хорошо детектить плагиат. Для их SFT не нашла годных данных, а генерить свои, увы, дороговато. Попробовала исправить это тем, что сначала использовала AST, а потом эмбеддинги, чтобы бвло несколько слоев фильтрации. Замеры, к сожалению, не делала -- только \"на глаз\" (хотя стоило, да). В итоге мне показалось, что наиболее удачный вариант это построить AST НАД дизассемблированием. По субъективным ощущениям для поставленных целей это наиболее разумный и эффективный вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dis\n",
    "import ast\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "class BytecodeNode(ast.AST):\n",
    "    ''' \n",
    "    Класс нужен, чтобы байтовый код отдавать в AST как ноды\n",
    "    '''\n",
    "\n",
    "    _fields = ['instruction', 'arguments']\n",
    "\n",
    "    def __init__(self, instruction, arguments):\n",
    "        self.instruction = instruction\n",
    "        self.arguments = arguments or []\n",
    "\n",
    "\n",
    "def disassemble_code(source_code):\n",
    "    ''' \n",
    "    ну тут делаем просто дизассемблинг исходного кода\n",
    "    '''\n",
    "    try:\n",
    "        compiled_code = compile(source_code, '<string>', 'exec')\n",
    "        return list(dis.get_instructions(compiled_code))\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "\n",
    "def bytecode_to_ast(bytecode):\n",
    "    ''' \n",
    "    байткод подаем в AST, чтобы построить дерево\n",
    "    '''\n",
    "    root = BytecodeNode('ROOT')\n",
    "    for instruction in bytecode:\n",
    "        root.arguments.append(BytecodeNode(instruction.opname, [str(instruction.argval) or \"\"]))\n",
    "    return root\n",
    "\n",
    "\n",
    "def ast_to_text(node, indent):\n",
    "    ''' \n",
    "    теперь то, что получили в AST, превратим в строчку, чтобы можно было превратить ее в вектор\n",
    "    '''\n",
    "    result = \"  \" * indent + f\"{node.instruction}(\"\n",
    "    result += \", \".join(\n",
    "        ast_to_text(arg, indent=0).strip() if isinstance(arg, BytecodeNode) else str(arg)\n",
    "        for arg in node.arguments\n",
    "    )\n",
    "    result += \")\\n\"\n",
    "    for child in getattr(node, 'arguments', []):\n",
    "        if isinstance(child, BytecodeNode):\n",
    "            result += ast_to_text(child, indent + 1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cluster_bytecode_asts(asts, eps = 0.5, min_samples = 2):\n",
    "    ''' \n",
    "    вот тут важно: я строчку AST превращаю в вектор tf-idf-векторайзером предобученным, чтобы можно было посчитать косинусное расстояние\n",
    "    потом считаю косинусное расстояние и кластеризую дбсканом на косинусных расстояниях \n",
    "    '''\n",
    "    texts = [ast_to_text(ast) for ast in asts]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(texts)\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "    distance_matrix[distance_matrix < 0] = 0\n",
    "\n",
    "    clustering = DBSCAN(metric='precomputed', eps=eps, min_samples=min_samples)\n",
    "    return clustering.fit_predict(distance_matrix), similarity_matrix\n",
    "\n",
    "\n",
    "def read_files_from_directory(directory_path: str):\n",
    "    \"\"\"Reads Python files from a directory.\"\"\"\n",
    "    return {\n",
    "        filename: open(os.path.join(directory_path, filename), 'r', encoding='utf-8').read()\n",
    "        for filename in os.listdir(directory_path) if filename.endswith('.py')\n",
    "    }\n",
    "\n",
    "\n",
    "def display_clusters_as_graph(filenames, similarity_matrix, clusters):\n",
    "    ''' \n",
    "    ОЧЕНЬ СТРЕМНАЯ ЧАСТЬ ДЛЯ ВИЗУАЛИЗАЦИИ ГРАФА\n",
    "    '''\n",
    "    G = nx.Graph()\n",
    "    for i, file1 in enumerate(filenames):\n",
    "        G.add_node(file1, cluster=clusters[i])\n",
    "        for j, file2 in enumerate(filenames):\n",
    "            if i != j and similarity_matrix[i][j] > 0.5:\n",
    "                G.add_edge(file1, file2, weight=similarity_matrix[i][j])\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    cluster_colors = {c: f\"rgb({(hash(c) % 256)}, {(hash(c + 1) % 256)}, {(hash(c + 2) % 256)})\" for c in set(clusters)}\n",
    "\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), mode='lines')\n",
    "\n",
    "    node_x, node_y, node_text, node_color = [], [], [], []\n",
    "    for node, cluster in nx.get_node_attributes(G, 'cluster').items():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_text.append(f\"{node} (Cluster {cluster})\")\n",
    "        node_color.append(cluster_colors[cluster])\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y, mode='markers', text=node_text,\n",
    "        marker=dict(colorscale='Viridis', size=10, color=node_color))\n",
    "\n",
    "    go.Figure(data=[edge_trace, node_trace], layout=go.Layout(\n",
    "        title=\"Списывальщики Всея Руси\", showlegend=False, hovermode='closest'\n",
    "    )).show()\n",
    "\n",
    "\n",
    "def all_proccess(directory_path, eps = 0.5, min_samples = 2):\n",
    "    ''' \n",
    "    Собираем это чудище по лоскуткам\n",
    "\n",
    "    eps = это эпсилон в дбскане, можно менять ручками\n",
    "    min_samples = минимальное количество сэмплов в одном кластере\n",
    "\n",
    "    '''\n",
    "\n",
    "    files = read_files_from_directory(directory_path)\n",
    "    if not files:\n",
    "        print(\"файлов неть, пожалуйста, положите сюда что-нибудь для тестов\")\n",
    "        return\n",
    "\n",
    "    filenames = list(files.keys())\n",
    "    bytecodes = {fname: disassemble_code(code) for fname, code in files.items()}\n",
    "    asts = {fname: bytecode_to_ast(bytecodes[fname]) for fname in filenames}\n",
    "\n",
    "    clusters, similarity_matrix = cluster_bytecode_asts(list(asts.values()), eps, min_samples)\n",
    "    display_clusters_as_graph(filenames, similarity_matrix, clusters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"test_files\" \n",
    "all_proccess(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"tests\" \n",
    "all_proccess(directory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
